{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github link -> https://github.com/KyriakiPot/ML---Rock-Paper-Scissors/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 300\n",
    "height = 200\n",
    "p1 = 0.5\n",
    "p2 = 0.5\n",
    "random_state = 0\n",
    "#number of games\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    class_names = ['rock', 'scissors', 'paper']\n",
    "    \n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(folder_path, class_name)\n",
    "        print(f'current class_path is {class_path}')\n",
    "\n",
    "        for image_file in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Resize the image to 30x30\n",
    "            image = cv2.resize(image, image_size)\n",
    "\n",
    "            # Normalize pixel values to be between 0 and 1\n",
    "            image = image / 255.0\n",
    "            \n",
    "            # Flatten the 2D array to 1D\n",
    "            image_vector = image.flatten()\n",
    "\n",
    "            # Append the image to the list\n",
    "            images.append(image_vector)\n",
    "\n",
    "            # Append the label for the current class\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current class_path is dataset\\rock\n",
      "current class_path is dataset\\scissors\n",
      "current class_path is dataset\\paper\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'dataset'\n",
    "images, labels = load_images(folder_path, (width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2188, 60000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findImageFromLabel(images, labels, label):\n",
    "    indices = np.where(labels == label)[0]\n",
    "    selected_images = images[indices]\n",
    "    selected_labels = labels[indices]\n",
    "    \n",
    "    return train_test_split(selected_images, selected_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1749, 439, 1749, 439, 2188)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scissors, X_test_scissors, y_train_scissors, y_test_scissors = findImageFromLabel(images, labels, 1)\n",
    "X_train_rock, X_test_rock, y_train_rock, y_test_rock = findImageFromLabel(images, labels, 0)\n",
    "X_train_paper, X_test_paper, y_train_paper, y_test_paper = findImageFromLabel(images, labels, 2)\n",
    "\n",
    "#Split train and test set\n",
    "X_train = [] \n",
    "X_test = [] \n",
    "y_train = [] \n",
    "y_test = []\n",
    "X_all = []\n",
    "\n",
    "X_train.extend(X_train_scissors)\n",
    "X_train.extend(X_train_rock)\n",
    "X_train.extend(X_train_paper)\n",
    "\n",
    "X_test.extend(X_test_scissors)\n",
    "X_test.extend(X_test_rock)\n",
    "X_test.extend(X_test_paper)\n",
    "\n",
    "y_train.extend(y_train_scissors)\n",
    "y_train.extend(y_train_rock)\n",
    "y_train.extend(y_train_paper)\n",
    "\n",
    "y_test.extend(y_test_scissors)\n",
    "y_test.extend(y_test_rock)\n",
    "y_test.extend(y_test_paper)\n",
    "\n",
    "X_all.extend(X_train)\n",
    "X_all.extend(X_test)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test), len(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA()\n",
    "images_pca = pca.fit_transform(X_all)\n",
    "\n",
    "# Visualize explained variance ratio\n",
    "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(explained_variance_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPca(image_dataset, num_components):\n",
    "    # Apply PCA for dimensionality reduction and choose num of components based on explained variance ratio\n",
    "    pca = PCA(n_components=num_components)\n",
    "    images_pca = pca.fit_transform(image_dataset)\n",
    "\n",
    "    # Visualize explained variance ratio\n",
    "    explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(explained_variance_ratio)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.show()\n",
    "\n",
    "    return images_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_components = 600  \n",
    "X_all_pca = applyPca(X_all, num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1749, 439)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = int(0.8 * len(X_all_pca)) - 1\n",
    "\n",
    "X_train_pca = X_all_pca[:split_index]\n",
    "X_test_pca = X_all_pca[split_index:]\n",
    "\n",
    "len(X_train_pca), len(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModel(X_train, X_test, y_train, y_test, model): \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 74.94%\n",
      "Logistic Regression Precision: 74.95%\n",
      "Logistic Regression Recall: 74.94%\n",
      "Logistic Regression F1: 74.91%\n",
      "Best Parameters: {'C': 0.01, 'solver': 'newton-cholesky'}\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "logistic_reg_param = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "logistic_reg_model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "logistic_reg_search = GridSearchCV(logistic_reg_model, logistic_reg_param, cv=5)\n",
    "\n",
    "lr_accuracy, lr_precision, lr_recall, lr_f1 = applyModel(X_train_pca, X_test_pca, y_train, y_test, logistic_reg_search)\n",
    "\n",
    "print(f'Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%')\n",
    "print(f'Logistic Regression Precision: {lr_precision * 100:.2f}%')\n",
    "print(f'Logistic Regression Recall: {lr_recall * 100:.2f}%')\n",
    "print(f'Logistic Regression F1: {lr_f1 * 100:.2f}%')\n",
    "\n",
    "print(f'Best Parameters: {logistic_reg_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 87.70%\n",
      "Random Forest Precision: 87.79%\n",
      "Random Forest Recall: 87.70%\n",
      "Random Forest F1: 87.70%\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "random_forest_param = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "random_forest_search = GridSearchCV(RandomForestClassifier(random_state=random_state), random_forest_param, cv=10)\n",
    "\n",
    "rf_accuracy, rf_precision, rf_recall, rf_f1 = applyModel(X_train_pca, X_test_pca, y_train, y_test, random_forest_search)\n",
    "\n",
    "print(f'Random Forest Accuracy: {rf_accuracy * 100:.2f}%')\n",
    "print(f'Random Forest Precision: {rf_precision * 100:.2f}%')\n",
    "print(f'Random Forest Recall: {rf_recall * 100:.2f}%')\n",
    "print(f'Random Forest F1: {rf_f1 * 100:.2f}%')\n",
    "\n",
    "print(f'Best Parameters: {random_forest_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 89.52%\n",
      "SVM Precision: 90.13%\n",
      "SVM Recall: 89.52%\n",
      "SVM F1: 89.53%\n",
      "Best Parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(random_state=random_state), svm_param_grid, cv=10)\n",
    "svm_accuracy, svm_precision, svm_recall, svm_f1 = applyModel(X_train_pca, X_test_pca, y_train, y_test, svm_grid_search)\n",
    "\n",
    "print(f'SVM Accuracy: {svm_accuracy * 100:.2f}%')\n",
    "print(f'SVM Precision: {svm_precision * 100:.2f}%')\n",
    "print(f'SVM Recall: {svm_recall * 100:.2f}%')\n",
    "print(f'SVM F1: {svm_f1 * 100:.2f}%')\n",
    "\n",
    "print(f'Best Parameters: {svm_grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 87.70%\n",
      "Random Forest Precision: 88.52%\n",
      "Random Forest Recall: 87.70%\n",
      "Random Forest F1: 87.74%\n",
      "Best Parameters: {'n_neighbors': 10, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "k_squared = np.sqrt(len(X_train_pca))\n",
    "k_rounded = np.round(k_squared).astype(int)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [10, 20, k_rounded],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=10)\n",
    "\n",
    "knn_accuracy, knn_precision, knn_recall, knn_f1 = applyModel(X_train_pca, X_test_pca, y_train, y_test, knn_grid_search)\n",
    "\n",
    "print(f'Random Forest Accuracy: {knn_accuracy * 100:.2f}%')\n",
    "print(f'Random Forest Precision: {knn_precision * 100:.2f}%')\n",
    "print(f'Random Forest Recall: {knn_recall * 100:.2f}%')\n",
    "print(f'Random Forest F1: {knn_f1 * 100:.2f}%')\n",
    "\n",
    "print(f'Best Parameters: {knn_grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Shape depends on the PCA\n",
    "input_shape = (num_components, 1, 1)\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "from tensorflow.python.framework import random_seed\n",
    "random_seed.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 598, 1, 32)        128       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 299, 1, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 297, 1, 64)        6208      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 148, 1, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9472)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1212544   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219267 (4.65 MB)\n",
      "Trainable params: 1219267 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\kyriaki.potamopoulou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "44/44 [==============================] - 2s 11ms/step - loss: 0.1042 - accuracy: 0.5461 - val_loss: 0.1315 - val_accuracy: 0.0829\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0496 - accuracy: 0.8349 - val_loss: 0.0840 - val_accuracy: 0.6371\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0217 - accuracy: 0.9271 - val_loss: 0.0538 - val_accuracy: 0.7829\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0104 - accuracy: 0.9714 - val_loss: 0.0719 - val_accuracy: 0.7029\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0068 - accuracy: 0.9814 - val_loss: 0.0383 - val_accuracy: 0.8571\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9907 - val_loss: 0.0579 - val_accuracy: 0.7686\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9907 - val_loss: 0.0557 - val_accuracy: 0.7886\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9914 - val_loss: 0.0695 - val_accuracy: 0.7286\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9921 - val_loss: 0.0346 - val_accuracy: 0.8686\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 0.9929 - val_loss: 0.0324 - val_accuracy: 0.8686\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 0.9929 - val_loss: 0.0655 - val_accuracy: 0.7486\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0018 - accuracy: 0.9950 - val_loss: 0.0265 - val_accuracy: 0.9029\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9964 - val_loss: 0.0214 - val_accuracy: 0.9229\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9979 - val_loss: 0.0528 - val_accuracy: 0.7971\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.8502e-04 - accuracy: 0.9979 - val_loss: 0.0420 - val_accuracy: 0.8257\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.8720e-04 - accuracy: 0.9979 - val_loss: 0.0474 - val_accuracy: 0.8114\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.4605e-04 - accuracy: 0.9979 - val_loss: 0.0530 - val_accuracy: 0.7857\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 9.5527e-04 - accuracy: 0.9971 - val_loss: 0.0639 - val_accuracy: 0.7571\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2242e-04 - accuracy: 0.9986 - val_loss: 0.0527 - val_accuracy: 0.8057\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.1527e-04 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.8257\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9837e-04 - accuracy: 0.9986 - val_loss: 0.0524 - val_accuracy: 0.8029\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.9496e-04 - accuracy: 0.9986 - val_loss: 0.0515 - val_accuracy: 0.8057\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9258e-04 - accuracy: 0.9986 - val_loss: 0.0523 - val_accuracy: 0.8057\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9075e-04 - accuracy: 0.9986 - val_loss: 0.0514 - val_accuracy: 0.8086\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8927e-04 - accuracy: 0.9986 - val_loss: 0.0515 - val_accuracy: 0.8086\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8805e-04 - accuracy: 0.9986 - val_loss: 0.0519 - val_accuracy: 0.8057\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8661e-04 - accuracy: 0.9986 - val_loss: 0.0523 - val_accuracy: 0.8057\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8539e-04 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.8057\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8370e-04 - accuracy: 0.9986 - val_loss: 0.0486 - val_accuracy: 0.8200\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 6.0483e-04 - accuracy: 0.9986 - val_loss: 0.0497 - val_accuracy: 0.8114\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.9469e-04 - accuracy: 0.9986 - val_loss: 0.0527 - val_accuracy: 0.7971\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8817e-04 - accuracy: 0.9986 - val_loss: 0.0548 - val_accuracy: 0.7943\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8631e-04 - accuracy: 0.9986 - val_loss: 0.0556 - val_accuracy: 0.7943\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8518e-04 - accuracy: 0.9986 - val_loss: 0.0548 - val_accuracy: 0.7943\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8431e-04 - accuracy: 0.9986 - val_loss: 0.0549 - val_accuracy: 0.7943\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8351e-04 - accuracy: 0.9986 - val_loss: 0.0529 - val_accuracy: 0.7971\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8302e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7943\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8251e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7943\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8206e-04 - accuracy: 0.9986 - val_loss: 0.0540 - val_accuracy: 0.7971\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8169e-04 - accuracy: 0.9986 - val_loss: 0.0540 - val_accuracy: 0.7971\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8139e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8111e-04 - accuracy: 0.9986 - val_loss: 0.0534 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8086e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.7971\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8060e-04 - accuracy: 0.9986 - val_loss: 0.0537 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8040e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8022e-04 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8002e-04 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7986e-04 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7970e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7956e-04 - accuracy: 0.9986 - val_loss: 0.0540 - val_accuracy: 0.7971\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7944e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.7971\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7933e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.7971\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7920e-04 - accuracy: 0.9986 - val_loss: 0.0540 - val_accuracy: 0.7971\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7909e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7898e-04 - accuracy: 0.9986 - val_loss: 0.0536 - val_accuracy: 0.7971\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7889e-04 - accuracy: 0.9986 - val_loss: 0.0537 - val_accuracy: 0.7971\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7880e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.7971\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7872e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7864e-04 - accuracy: 0.9986 - val_loss: 0.0537 - val_accuracy: 0.7971\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7857e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.7971\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7849e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7842e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7835e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.7971\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7829e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7823e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.7971\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7818e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.7971\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7812e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7806e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.7971\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7801e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7796e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7792e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7788e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7784e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7780e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7775e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7772e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7768e-04 - accuracy: 0.9986 - val_loss: 0.0541 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7765e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7761e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7758e-04 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.7971\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7755e-04 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.8000\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7752e-04 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.7971\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.7749e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 84/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7746e-04 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.7971\n",
      "Epoch 85/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7743e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.7971\n",
      "Epoch 86/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7741e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 87/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7738e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 88/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7736e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 89/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7734e-04 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.7971\n",
      "Epoch 90/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7731e-04 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.7971\n",
      "Epoch 91/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7729e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 92/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7727e-04 - accuracy: 0.9986 - val_loss: 0.0544 - val_accuracy: 0.7971\n",
      "Epoch 93/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7725e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 94/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7723e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.7971\n",
      "Epoch 95/100\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.7721e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.7971\n",
      "Epoch 96/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7719e-04 - accuracy: 0.9986 - val_loss: 0.0547 - val_accuracy: 0.7971\n",
      "Epoch 97/100\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7718e-04 - accuracy: 0.9986 - val_loss: 0.0547 - val_accuracy: 0.7971\n",
      "Epoch 98/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7716e-04 - accuracy: 0.9986 - val_loss: 0.0548 - val_accuracy: 0.7971\n",
      "Epoch 99/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7714e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "Epoch 100/100\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7713e-04 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.7971\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9431\n",
      "CNN Accuracy: 94.31%\n"
     ]
    }
   ],
   "source": [
    "# Create a Convolutional Neural Network\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 1), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 1), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='huber', metrics=['accuracy'])\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the CNN model\n",
    "cnn_model.fit(X_train_pca[..., np.newaxis], y_train_one_hot, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_pca[..., np.newaxis], y_test_one_hot)\n",
    "print(f'CNN Accuracy: {cnn_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 10ms/step - loss: 0.0765 - accuracy: 0.6805 - val_loss: 0.0945 - val_accuracy: 0.5771\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0282 - accuracy: 0.9006 - val_loss: 0.0907 - val_accuracy: 0.6229\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.9514 - val_loss: 0.0393 - val_accuracy: 0.8571\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9786 - val_loss: 0.0577 - val_accuracy: 0.7971\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 0.9921 - val_loss: 0.0448 - val_accuracy: 0.8371\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0021 - accuracy: 0.9950 - val_loss: 0.0528 - val_accuracy: 0.8057\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9950 - val_loss: 0.0436 - val_accuracy: 0.8400\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9957 - val_loss: 0.0659 - val_accuracy: 0.7486\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 0.9964 - val_loss: 0.0602 - val_accuracy: 0.7714\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.2813e-04 - accuracy: 0.9986 - val_loss: 0.0454 - val_accuracy: 0.8200\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.4368e-04 - accuracy: 0.9993 - val_loss: 0.0422 - val_accuracy: 0.8429\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.0793e-04 - accuracy: 0.9993 - val_loss: 0.0449 - val_accuracy: 0.8343\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.8952e-04 - accuracy: 0.9993 - val_loss: 0.0450 - val_accuracy: 0.8371\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.7849e-04 - accuracy: 0.9993 - val_loss: 0.0435 - val_accuracy: 0.8457\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.1000e-04 - accuracy: 0.9993 - val_loss: 0.0528 - val_accuracy: 0.8000\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.1814e-04 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.8343\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.5695e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.8286\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.6369e-05 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.8371\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.2623e-05 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.8343\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.8878e-05 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.8371\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.6201e-05 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.8343\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.4546e-05 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.8400\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.3054e-05 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.8343\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.1709e-05 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.8371\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.0664e-05 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.8371\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 9.7957e-06 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.8371\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 9.0855e-06 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.8343\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 8.4573e-06 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.8371\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.8394e-06 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.8371\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.2980e-06 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.8314\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.8307e-06 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.8343\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 6.4721e-06 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.8314\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 6.0770e-06 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.8314\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.7081e-06 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.8371\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 5.4001e-06 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.8314\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.1364e-06 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.8371\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8518e-06 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.8314\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5828e-06 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.8371\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.3608e-06 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.8343\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.1534e-06 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.8343\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.9446e-06 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.8343\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.7618e-06 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.8343\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.5808e-06 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.8343\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.4226e-06 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.8343\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.2806e-06 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.8343\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.1483e-06 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.8343\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.0145e-06 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.8343\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.8873e-06 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.8314\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.7790e-06 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 0.8314\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.6529e-06 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.8371\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.5539e-06 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.8371\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4573e-06 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.8314\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.3564e-06 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.8314\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.2671e-06 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.8314\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1843e-06 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.8314\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1051e-06 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.8371\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.0222e-06 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.8314\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.9546e-06 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.8314\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.9048e-06 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.8314\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.8111e-06 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.8343\n",
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 9ms/step - loss: 0.0889 - accuracy: 0.5854 - val_loss: 0.0891 - val_accuracy: 0.5857\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.8878 - val_loss: 0.1473 - val_accuracy: 0.3543\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.9535 - val_loss: 0.0506 - val_accuracy: 0.8000\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 0.9778 - val_loss: 0.0651 - val_accuracy: 0.7486\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0046 - accuracy: 0.9878 - val_loss: 0.0407 - val_accuracy: 0.8457\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 0.9950 - val_loss: 0.0548 - val_accuracy: 0.7914\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 0.9964 - val_loss: 0.0557 - val_accuracy: 0.8000\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 0.9971 - val_loss: 0.0529 - val_accuracy: 0.8057\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 0.9971 - val_loss: 0.0678 - val_accuracy: 0.7400\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.0196e-04 - accuracy: 0.9986 - val_loss: 0.0562 - val_accuracy: 0.7914\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.4600e-04 - accuracy: 0.9986 - val_loss: 0.0533 - val_accuracy: 0.8114\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.0504e-04 - accuracy: 0.9986 - val_loss: 0.0499 - val_accuracy: 0.8171\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2842e-04 - accuracy: 0.9986 - val_loss: 0.0478 - val_accuracy: 0.8257\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9295e-04 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.8029\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 5.3981e-04 - accuracy: 0.9986 - val_loss: 0.0490 - val_accuracy: 0.8257\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9734e-04 - accuracy: 0.9986 - val_loss: 0.0486 - val_accuracy: 0.8200\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.0628e-04 - accuracy: 0.9986 - val_loss: 0.0495 - val_accuracy: 0.8229\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.0037e-04 - accuracy: 0.9986 - val_loss: 0.0509 - val_accuracy: 0.8171\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9745e-04 - accuracy: 0.9986 - val_loss: 0.0508 - val_accuracy: 0.8143\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9502e-04 - accuracy: 0.9986 - val_loss: 0.0497 - val_accuracy: 0.8200\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.9297e-04 - accuracy: 0.9986 - val_loss: 0.0506 - val_accuracy: 0.8200\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9079e-04 - accuracy: 0.9986 - val_loss: 0.0507 - val_accuracy: 0.8171\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8935e-04 - accuracy: 0.9986 - val_loss: 0.0506 - val_accuracy: 0.8171\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5789e-04 - accuracy: 0.9986 - val_loss: 0.0481 - val_accuracy: 0.8286\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.1716e-04 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.8114\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9470e-04 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.8114\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8879e-04 - accuracy: 0.9986 - val_loss: 0.0510 - val_accuracy: 0.8114\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8755e-04 - accuracy: 0.9986 - val_loss: 0.0510 - val_accuracy: 0.8114\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8641e-04 - accuracy: 0.9986 - val_loss: 0.0509 - val_accuracy: 0.8114\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8555e-04 - accuracy: 0.9986 - val_loss: 0.0520 - val_accuracy: 0.8114\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8488e-04 - accuracy: 0.9986 - val_loss: 0.0512 - val_accuracy: 0.8114\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8429e-04 - accuracy: 0.9986 - val_loss: 0.0521 - val_accuracy: 0.8114\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8368e-04 - accuracy: 0.9986 - val_loss: 0.0520 - val_accuracy: 0.8114\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8322e-04 - accuracy: 0.9986 - val_loss: 0.0511 - val_accuracy: 0.8143\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8273e-04 - accuracy: 0.9986 - val_loss: 0.0520 - val_accuracy: 0.8114\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8235e-04 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.8114\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8199e-04 - accuracy: 0.9986 - val_loss: 0.0525 - val_accuracy: 0.8114\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8164e-04 - accuracy: 0.9986 - val_loss: 0.0503 - val_accuracy: 0.8200\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8134e-04 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.8114\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8102e-04 - accuracy: 0.9986 - val_loss: 0.0504 - val_accuracy: 0.8200\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8070e-04 - accuracy: 0.9986 - val_loss: 0.0514 - val_accuracy: 0.8114\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.8044e-04 - accuracy: 0.9986 - val_loss: 0.0507 - val_accuracy: 0.8171\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7992e-04 - accuracy: 0.9986 - val_loss: 0.0513 - val_accuracy: 0.8114\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.7904e-04 - accuracy: 0.9986 - val_loss: 0.0498 - val_accuracy: 0.8229\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.3549e-04 - accuracy: 0.9986 - val_loss: 0.0665 - val_accuracy: 0.7571\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.9571 - val_loss: 0.0123 - val_accuracy: 0.9514\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9628 - val_loss: 0.0647 - val_accuracy: 0.7629\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9907 - val_loss: 0.1067 - val_accuracy: 0.6000\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 0.9907 - val_loss: 0.0812 - val_accuracy: 0.7000\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.3900e-04 - accuracy: 0.9993 - val_loss: 0.0680 - val_accuracy: 0.7657\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7801e-04 - accuracy: 0.9993 - val_loss: 0.0492 - val_accuracy: 0.8229\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 9.5459e-05 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 0.7771\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 9.9609e-05 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.7629\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.9894e-05 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.7571\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.3362e-06 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.7686\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.5954e-06 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.7743\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.2501e-06 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.7800\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.0153e-06 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.7800\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.8570e-06 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.7829\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.7311e-06 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.7829\n",
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 10ms/step - loss: 0.0929 - accuracy: 0.5761 - val_loss: 0.1041 - val_accuracy: 0.4743\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0361 - accuracy: 0.8735 - val_loss: 0.1165 - val_accuracy: 0.4971\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0186 - accuracy: 0.9342 - val_loss: 0.0359 - val_accuracy: 0.8514\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0075 - accuracy: 0.9807 - val_loss: 0.0655 - val_accuracy: 0.7371\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0037 - accuracy: 0.9907 - val_loss: 0.0276 - val_accuracy: 0.8886\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 0.9950 - val_loss: 0.0794 - val_accuracy: 0.6829\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9964 - val_loss: 0.0518 - val_accuracy: 0.8029\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 8.1211e-04 - accuracy: 0.9986 - val_loss: 0.0408 - val_accuracy: 0.8400\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.4985e-04 - accuracy: 0.9986 - val_loss: 0.0400 - val_accuracy: 0.8400\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.3492e-04 - accuracy: 0.9986 - val_loss: 0.0529 - val_accuracy: 0.8057\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.9904e-04 - accuracy: 0.9986 - val_loss: 0.0482 - val_accuracy: 0.8200\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5481e-04 - accuracy: 0.9986 - val_loss: 0.0453 - val_accuracy: 0.8257\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.1445e-04 - accuracy: 0.9986 - val_loss: 0.0392 - val_accuracy: 0.8486\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.4001e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.8400\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.6337e-05 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.8286\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.6307e-05 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.8286\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.0850e-05 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.8314\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.6651e-05 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.8200\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.3913e-05 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.8286\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1160e-05 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.8286\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.9255e-05 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.8286\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.7499e-05 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.8257\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.5754e-05 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.8171\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.4371e-05 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.8286\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.3226e-05 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.8314\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.2173e-05 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.8286\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 1.1393e-05 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.8286\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.0627e-05 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 0.8314\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 9.8837e-06 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.8314\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 9.0767e-06 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.8257\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 8.5404e-06 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.8286\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.9986e-06 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.8200\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.4624e-06 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.8286\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.0734e-06 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.8314\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.6514e-06 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.8229\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.3105e-06 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.8257\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.9241e-06 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.8171\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 5.5825e-06 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.8343\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2978e-06 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.8257\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.0294e-06 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.8286\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.7797e-06 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.8200\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.5666e-06 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.8257\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 4.3270e-06 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.8286\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.1229e-06 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.8171\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.9176e-06 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.8257\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.7585e-06 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.8257\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.5718e-06 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.8229\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.4181e-06 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.8229\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.2763e-06 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.8200\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.1269e-06 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.8229\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.9825e-06 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.8257\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.8678e-06 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.8229\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.7412e-06 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.8200\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.6291e-06 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.8229\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.5319e-06 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.8257\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4306e-06 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.8229\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.3337e-06 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.8171\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.2465e-06 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.8171\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1550e-06 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.8200\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.0711e-06 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.8257\n",
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 10ms/step - loss: 0.0842 - accuracy: 0.6212 - val_loss: 0.0740 - val_accuracy: 0.7057\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0319 - accuracy: 0.8885 - val_loss: 0.1142 - val_accuracy: 0.4886\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0136 - accuracy: 0.9593 - val_loss: 0.0413 - val_accuracy: 0.8257\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9843 - val_loss: 0.0391 - val_accuracy: 0.8343\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9893 - val_loss: 0.0300 - val_accuracy: 0.8857\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9957 - val_loss: 0.0930 - val_accuracy: 0.6314\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9971 - val_loss: 0.0633 - val_accuracy: 0.7457\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 8.3605e-04 - accuracy: 0.9986 - val_loss: 0.0735 - val_accuracy: 0.7086\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 6.3268e-04 - accuracy: 0.9986 - val_loss: 0.0499 - val_accuracy: 0.8086\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.4712e-04 - accuracy: 0.9986 - val_loss: 0.0491 - val_accuracy: 0.8114\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.3396e-04 - accuracy: 0.9986 - val_loss: 0.0456 - val_accuracy: 0.8257\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2714e-04 - accuracy: 0.9986 - val_loss: 0.0411 - val_accuracy: 0.8343\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.2799e-04 - accuracy: 0.9986 - val_loss: 0.0440 - val_accuracy: 0.8286\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 6.8559e-04 - accuracy: 0.9986 - val_loss: 0.0848 - val_accuracy: 0.6600\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 0.9979 - val_loss: 0.0303 - val_accuracy: 0.8829\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 5.3639e-04 - accuracy: 0.9993 - val_loss: 0.0823 - val_accuracy: 0.6857\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 3.6574e-04 - accuracy: 0.9993 - val_loss: 0.0474 - val_accuracy: 0.8171\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.6812e-04 - accuracy: 0.9993 - val_loss: 0.0487 - val_accuracy: 0.8200\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.5789e-04 - accuracy: 0.9993 - val_loss: 0.0493 - val_accuracy: 0.8171\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.5441e-04 - accuracy: 0.9993 - val_loss: 0.0481 - val_accuracy: 0.8171\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.5239e-04 - accuracy: 0.9993 - val_loss: 0.0496 - val_accuracy: 0.8143\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.5084e-04 - accuracy: 0.9993 - val_loss: 0.0497 - val_accuracy: 0.8114\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4958e-04 - accuracy: 0.9993 - val_loss: 0.0496 - val_accuracy: 0.8114\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4863e-04 - accuracy: 0.9993 - val_loss: 0.0498 - val_accuracy: 0.8114\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4769e-04 - accuracy: 0.9993 - val_loss: 0.0494 - val_accuracy: 0.8143\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4698e-04 - accuracy: 0.9993 - val_loss: 0.0500 - val_accuracy: 0.8114\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4630e-04 - accuracy: 0.9993 - val_loss: 0.0497 - val_accuracy: 0.8114\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4575e-04 - accuracy: 0.9993 - val_loss: 0.0500 - val_accuracy: 0.8114\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4521e-04 - accuracy: 0.9993 - val_loss: 0.0501 - val_accuracy: 0.8114\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4474e-04 - accuracy: 0.9993 - val_loss: 0.0500 - val_accuracy: 0.8114\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4433e-04 - accuracy: 0.9993 - val_loss: 0.0505 - val_accuracy: 0.8114\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.4391e-04 - accuracy: 0.9993 - val_loss: 0.0506 - val_accuracy: 0.8114\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4337e-04 - accuracy: 0.9993 - val_loss: 0.0507 - val_accuracy: 0.8114\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4298e-04 - accuracy: 0.9993 - val_loss: 0.0506 - val_accuracy: 0.8086\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4231e-04 - accuracy: 0.9993 - val_loss: 0.0508 - val_accuracy: 0.8086\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4461e-04 - accuracy: 0.9993 - val_loss: 0.0508 - val_accuracy: 0.8114\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 5.5442e-06 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.8114\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.8789e-06 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.8086\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.5746e-06 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.8057\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.2986e-06 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.8057\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 4.0653e-06 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.8057\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.8131e-06 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.8086\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.5884e-06 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.8086\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.4010e-06 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.8086\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.2429e-06 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.8057\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 3.1038e-06 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.8057\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.9701e-06 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.8086\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.8214e-06 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.8057\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.7014e-06 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.8057\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.5908e-06 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.8029\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4827e-06 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.8086\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.3890e-06 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.8057\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.2854e-06 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.8029\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1984e-06 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.8057\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.1214e-06 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.8057\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.0323e-06 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.8029\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.9543e-06 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.8029\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.8911e-06 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.8029\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.8139e-06 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.8029\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 1.7499e-06 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.8057\n",
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 11ms/step - loss: 0.0834 - accuracy: 0.6226 - val_loss: 0.0787 - val_accuracy: 0.6857\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0271 - accuracy: 0.9114 - val_loss: 0.0901 - val_accuracy: 0.6171\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0131 - accuracy: 0.9585 - val_loss: 0.0447 - val_accuracy: 0.8343\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9857 - val_loss: 0.0335 - val_accuracy: 0.8800\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 0.9943 - val_loss: 0.0373 - val_accuracy: 0.8600\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 0.9957 - val_loss: 0.0281 - val_accuracy: 0.8943\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 9.8603e-04 - accuracy: 0.9979 - val_loss: 0.0406 - val_accuracy: 0.8543\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 8.6632e-04 - accuracy: 0.9979 - val_loss: 0.0453 - val_accuracy: 0.8229\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 8.1359e-04 - accuracy: 0.9979 - val_loss: 0.0471 - val_accuracy: 0.8229\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.8743e-04 - accuracy: 0.9979 - val_loss: 0.0530 - val_accuracy: 0.8000\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.3454e-04 - accuracy: 0.9979 - val_loss: 0.0282 - val_accuracy: 0.9029\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 7.6962e-04 - accuracy: 0.9979 - val_loss: 0.0409 - val_accuracy: 0.8486\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 8.5740e-04 - accuracy: 0.9979 - val_loss: 0.0506 - val_accuracy: 0.7971\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 6.7873e-04 - accuracy: 0.9986 - val_loss: 0.0447 - val_accuracy: 0.8371\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 5.2755e-04 - accuracy: 0.9986 - val_loss: 0.0478 - val_accuracy: 0.8229\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 5.0467e-04 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.8343\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 5.2234e-04 - accuracy: 0.9986 - val_loss: 0.0451 - val_accuracy: 0.8400\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.9978e-04 - accuracy: 0.9986 - val_loss: 0.0470 - val_accuracy: 0.8257\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.9510e-04 - accuracy: 0.9986 - val_loss: 0.0474 - val_accuracy: 0.8257\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.9259e-04 - accuracy: 0.9986 - val_loss: 0.0471 - val_accuracy: 0.8286\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.9070e-04 - accuracy: 0.9986 - val_loss: 0.0484 - val_accuracy: 0.8200\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8946e-04 - accuracy: 0.9986 - val_loss: 0.0478 - val_accuracy: 0.8257\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.8806e-04 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.8257\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8705e-04 - accuracy: 0.9986 - val_loss: 0.0477 - val_accuracy: 0.8257\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8608e-04 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.8229\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 1s 12ms/step - loss: 4.8541e-04 - accuracy: 0.9986 - val_loss: 0.0477 - val_accuracy: 0.8229\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8471e-04 - accuracy: 0.9986 - val_loss: 0.0477 - val_accuracy: 0.8257\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.8407e-04 - accuracy: 0.9986 - val_loss: 0.0480 - val_accuracy: 0.8229\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8349e-04 - accuracy: 0.9986 - val_loss: 0.0478 - val_accuracy: 0.8229\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8297e-04 - accuracy: 0.9986 - val_loss: 0.0482 - val_accuracy: 0.8229\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8263e-04 - accuracy: 0.9986 - val_loss: 0.0483 - val_accuracy: 0.8229\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.8198e-04 - accuracy: 0.9986 - val_loss: 0.0486 - val_accuracy: 0.8200\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.8010e-04 - accuracy: 0.9986 - val_loss: 0.0478 - val_accuracy: 0.8257\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 4.8350e-04 - accuracy: 0.9986 - val_loss: 0.0424 - val_accuracy: 0.8400\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.5317e-04 - accuracy: 0.9993 - val_loss: 0.0530 - val_accuracy: 0.8086\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4491e-04 - accuracy: 0.9993 - val_loss: 0.0476 - val_accuracy: 0.8286\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4382e-04 - accuracy: 0.9993 - val_loss: 0.0491 - val_accuracy: 0.8171\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4327e-04 - accuracy: 0.9993 - val_loss: 0.0485 - val_accuracy: 0.8200\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4282e-04 - accuracy: 0.9993 - val_loss: 0.0494 - val_accuracy: 0.8143\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4248e-04 - accuracy: 0.9993 - val_loss: 0.0480 - val_accuracy: 0.8257\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 1s 12ms/step - loss: 2.4218e-04 - accuracy: 0.9993 - val_loss: 0.0490 - val_accuracy: 0.8171\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 1s 12ms/step - loss: 2.4193e-04 - accuracy: 0.9993 - val_loss: 0.0486 - val_accuracy: 0.8257\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4171e-04 - accuracy: 0.9993 - val_loss: 0.0487 - val_accuracy: 0.8257\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4149e-04 - accuracy: 0.9993 - val_loss: 0.0490 - val_accuracy: 0.8229\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 1s 12ms/step - loss: 2.4134e-04 - accuracy: 0.9993 - val_loss: 0.0492 - val_accuracy: 0.8200\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4115e-04 - accuracy: 0.9993 - val_loss: 0.0491 - val_accuracy: 0.8257\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4099e-04 - accuracy: 0.9993 - val_loss: 0.0487 - val_accuracy: 0.8257\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 2.4087e-04 - accuracy: 0.9993 - val_loss: 0.0486 - val_accuracy: 0.8257\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 2.4072e-04 - accuracy: 0.9993 - val_loss: 0.0494 - val_accuracy: 0.8229\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 2.4058e-04 - accuracy: 0.9993 - val_loss: 0.0487 - val_accuracy: 0.8257\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 2.4044e-04 - accuracy: 0.9993 - val_loss: 0.0488 - val_accuracy: 0.8257\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 2.4030e-04 - accuracy: 0.9993 - val_loss: 0.0488 - val_accuracy: 0.8257\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 2.4009e-04 - accuracy: 0.9993 - val_loss: 0.0491 - val_accuracy: 0.8257\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 2.4008e-04 - accuracy: 0.9993 - val_loss: 0.0506 - val_accuracy: 0.8171\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0191 - accuracy: 0.9314 - val_loss: 0.2775 - val_accuracy: 0.0057\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0244 - accuracy: 0.9071 - val_loss: 0.0297 - val_accuracy: 0.8829\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0055 - accuracy: 0.9800 - val_loss: 0.0994 - val_accuracy: 0.6314\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0047 - accuracy: 0.9793 - val_loss: 0.0291 - val_accuracy: 0.8914\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0037 - accuracy: 0.9836 - val_loss: 0.0302 - val_accuracy: 0.8943\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 8.0822e-04 - accuracy: 0.9971 - val_loss: 0.0463 - val_accuracy: 0.8257\n"
     ]
    }
   ],
   "source": [
    "def create_model(seed):\n",
    "    random_seed.set_seed(seed)\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=(3, 1), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 1), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])  \n",
    "    model.compile(optimizer='adam', loss='huber', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_models = 5\n",
    "\n",
    "#Create models\n",
    "models_list = [create_model(seed) for seed in range(num_models)]\n",
    "\n",
    "#Train models\n",
    "for i, model in enumerate(models_list):\n",
    "    model.fit(X_train_pca[..., np.newaxis], y_train_one_hot, epochs=60, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 3ms/step\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Ensemble CNN Accuracy: 94.08%\n"
     ]
    }
   ],
   "source": [
    "#Make predictions for each model\n",
    "predictions_list = [model.predict(X_test_pca[..., np.newaxis]) for model in models_list]\n",
    "\n",
    "#Ensemble predictions\n",
    "ensemble_predictions = np.argmax(np.sum(predictions_list, axis=0), axis=1)\n",
    "\n",
    "accuracy_majority_vote = np.sum(ensemble_predictions == y_test) / len(y_test)\n",
    "print(f'Ensemble CNN Accuracy: {accuracy_majority_vote * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_54 (Flatten)        (None, 600)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 128)               76928     \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85379 (333.51 KB)\n",
      "Trainable params: 85379 (333.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "44/44 [==============================] - 1s 4ms/step - loss: 0.8793 - accuracy: 0.6283 - val_loss: 1.3524 - val_accuracy: 0.3714\n",
      "Epoch 2/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8964 - val_loss: 1.0660 - val_accuracy: 0.5371\n",
      "Epoch 3/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9628 - val_loss: 1.0261 - val_accuracy: 0.5829\n",
      "Epoch 4/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9864 - val_loss: 1.1009 - val_accuracy: 0.5771\n",
      "Epoch 5/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9993 - val_loss: 1.1143 - val_accuracy: 0.6029\n",
      "Epoch 6/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 1.0920 - val_accuracy: 0.6229\n",
      "Epoch 7/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.1837 - val_accuracy: 0.6114\n",
      "Epoch 8/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.1816 - val_accuracy: 0.6229\n",
      "Epoch 9/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.2193 - val_accuracy: 0.6200\n",
      "Epoch 10/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2593 - val_accuracy: 0.6229\n",
      "Epoch 11/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2985 - val_accuracy: 0.6200\n",
      "Epoch 12/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3032 - val_accuracy: 0.6229\n",
      "Epoch 13/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.3257 - val_accuracy: 0.6200\n",
      "Epoch 14/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3574 - val_accuracy: 0.6200\n",
      "Epoch 15/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.6229\n",
      "Epoch 16/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3991 - val_accuracy: 0.6229\n",
      "Epoch 17/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4188 - val_accuracy: 0.6229\n",
      "Epoch 18/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.4402 - val_accuracy: 0.6229\n",
      "Epoch 19/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4606 - val_accuracy: 0.6257\n",
      "Epoch 20/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.4689 - val_accuracy: 0.6257\n",
      "Epoch 21/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 9.1058e-04 - accuracy: 1.0000 - val_loss: 1.4922 - val_accuracy: 0.6257\n",
      "Epoch 22/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 8.1283e-04 - accuracy: 1.0000 - val_loss: 1.5104 - val_accuracy: 0.6257\n",
      "Epoch 23/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 7.2973e-04 - accuracy: 1.0000 - val_loss: 1.5265 - val_accuracy: 0.6286\n",
      "Epoch 24/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 6.5812e-04 - accuracy: 1.0000 - val_loss: 1.5404 - val_accuracy: 0.6257\n",
      "Epoch 25/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 5.9556e-04 - accuracy: 1.0000 - val_loss: 1.5506 - val_accuracy: 0.6314\n",
      "Epoch 26/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 5.4127e-04 - accuracy: 1.0000 - val_loss: 1.5716 - val_accuracy: 0.6286\n",
      "Epoch 27/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 4.9418e-04 - accuracy: 1.0000 - val_loss: 1.5806 - val_accuracy: 0.6314\n",
      "Epoch 28/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 4.5227e-04 - accuracy: 1.0000 - val_loss: 1.5905 - val_accuracy: 0.6314\n",
      "Epoch 29/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 4.1517e-04 - accuracy: 1.0000 - val_loss: 1.6063 - val_accuracy: 0.6314\n",
      "Epoch 30/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 3.8223e-04 - accuracy: 1.0000 - val_loss: 1.6156 - val_accuracy: 0.6314\n",
      "Epoch 31/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 3.5239e-04 - accuracy: 1.0000 - val_loss: 1.6307 - val_accuracy: 0.6314\n",
      "Epoch 32/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 3.2638e-04 - accuracy: 1.0000 - val_loss: 1.6439 - val_accuracy: 0.6314\n",
      "Epoch 33/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 3.0250e-04 - accuracy: 1.0000 - val_loss: 1.6577 - val_accuracy: 0.6314\n",
      "Epoch 34/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.8123e-04 - accuracy: 1.0000 - val_loss: 1.6629 - val_accuracy: 0.6314\n",
      "Epoch 35/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.6225e-04 - accuracy: 1.0000 - val_loss: 1.6726 - val_accuracy: 0.6314\n",
      "Epoch 36/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.4461e-04 - accuracy: 1.0000 - val_loss: 1.6883 - val_accuracy: 0.6314\n",
      "Epoch 37/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.2863e-04 - accuracy: 1.0000 - val_loss: 1.7001 - val_accuracy: 0.6314\n",
      "Epoch 38/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.1406e-04 - accuracy: 1.0000 - val_loss: 1.7141 - val_accuracy: 0.6314\n",
      "Epoch 39/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.0068e-04 - accuracy: 1.0000 - val_loss: 1.7206 - val_accuracy: 0.6314\n",
      "Epoch 40/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.8859e-04 - accuracy: 1.0000 - val_loss: 1.7273 - val_accuracy: 0.6314\n",
      "Epoch 41/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.7733e-04 - accuracy: 1.0000 - val_loss: 1.7397 - val_accuracy: 0.6314\n",
      "Epoch 42/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.6717e-04 - accuracy: 1.0000 - val_loss: 1.7463 - val_accuracy: 0.6314\n",
      "Epoch 43/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.5748e-04 - accuracy: 1.0000 - val_loss: 1.7567 - val_accuracy: 0.6314\n",
      "Epoch 44/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.4877e-04 - accuracy: 1.0000 - val_loss: 1.7674 - val_accuracy: 0.6314\n",
      "Epoch 45/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.4063e-04 - accuracy: 1.0000 - val_loss: 1.7736 - val_accuracy: 0.6314\n",
      "Epoch 46/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.3301e-04 - accuracy: 1.0000 - val_loss: 1.7841 - val_accuracy: 0.6314\n",
      "Epoch 47/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.2595e-04 - accuracy: 1.0000 - val_loss: 1.7920 - val_accuracy: 0.6314\n",
      "Epoch 48/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.1935e-04 - accuracy: 1.0000 - val_loss: 1.7997 - val_accuracy: 0.6343\n",
      "Epoch 49/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.1344e-04 - accuracy: 1.0000 - val_loss: 1.8092 - val_accuracy: 0.6343\n",
      "Epoch 50/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.0760e-04 - accuracy: 1.0000 - val_loss: 1.8197 - val_accuracy: 0.6314\n",
      "Epoch 51/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.0220e-04 - accuracy: 1.0000 - val_loss: 1.8263 - val_accuracy: 0.6314\n",
      "Epoch 52/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 9.7264e-05 - accuracy: 1.0000 - val_loss: 1.8322 - val_accuracy: 0.6314\n",
      "Epoch 53/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 9.2507e-05 - accuracy: 1.0000 - val_loss: 1.8413 - val_accuracy: 0.6314\n",
      "Epoch 54/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 8.8193e-05 - accuracy: 1.0000 - val_loss: 1.8462 - val_accuracy: 0.6314\n",
      "Epoch 55/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 8.4129e-05 - accuracy: 1.0000 - val_loss: 1.8555 - val_accuracy: 0.6314\n",
      "Epoch 56/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 8.0251e-05 - accuracy: 1.0000 - val_loss: 1.8634 - val_accuracy: 0.6314\n",
      "Epoch 57/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 7.6585e-05 - accuracy: 1.0000 - val_loss: 1.8708 - val_accuracy: 0.6314\n",
      "Epoch 58/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 7.3200e-05 - accuracy: 1.0000 - val_loss: 1.8789 - val_accuracy: 0.6314\n",
      "Epoch 59/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 6.9942e-05 - accuracy: 1.0000 - val_loss: 1.8849 - val_accuracy: 0.6314\n",
      "Epoch 60/60\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 6.6873e-05 - accuracy: 1.0000 - val_loss: 1.8908 - val_accuracy: 0.6314\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.6960 - accuracy: 0.8519\n",
      "DNN Accuracy: 85.19%\n"
     ]
    }
   ],
   "source": [
    "# Create a Deep Neural Network\n",
    "dnn_model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "dnn_model.summary()\n",
    "\n",
    "# Train the CNN model\n",
    "dnn_model.fit(X_train_pca[..., np.newaxis], y_train_one_hot, epochs=60, validation_split=0.2)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test_pca[..., np.newaxis], y_test_one_hot)\n",
    "print(f'DNN Accuracy: {dnn_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical and Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flips(image, p1, p2):\n",
    "    image_like_structure = image.reshape((20, 30))\n",
    "\n",
    "    # Apply horizontal flip with probability p1\n",
    "    if np.random.rand() < p1:\n",
    "        image = cv2.flip(image_like_structure, 1).flatten()\n",
    "\n",
    "    # Apply vertical flip with probability p2\n",
    "    if np.random.rand() < p2:\n",
    "        image = cv2.flip(image_like_structure, 0).flatten()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image, mean, std_dev):\n",
    "    noise = np.random.normal(mean, std_dev, image.shape)\n",
    "    return np.clip(image + noise, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image(images, labels):\n",
    "    random_index = random.randint(0, len(images) - 1)\n",
    "\n",
    "    random_image = images[random_index]\n",
    "    random_label = labels[random_index]\n",
    "\n",
    "    return random_image, random_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImageLabel(random_image, random_label):    \n",
    "    if len(random_image) <= 0:\n",
    "        random_image, random_label = get_random_image(X_test_pca, y_test)\n",
    "    image_with_flips = apply_flips(random_image, p1, p2)\n",
    "    image_with_flips_noise = apply_flips(image_with_flips, p1, p2)\n",
    "\n",
    "    image_with_flips_noise = image_with_flips_noise.reshape((1, 600, 1, 1))\n",
    "\n",
    "    # Predict the label using the trained CNN model\n",
    "    prediction = cnn_model.predict(image_with_flips_noise)\n",
    "\n",
    "    # Get the predicted label (index with the maximum probability)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "\n",
    "    return random_label, predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestMove(predicted_label):\n",
    "    if(predicted_label == 0):\n",
    "        return 2\n",
    "    elif(predicted_label == 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Actual label: 2, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 1, Best move: 0\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 0, Best move: 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "Total gain: 105\n"
     ]
    }
   ],
   "source": [
    "gain = 0\n",
    "gain_list = []\n",
    "\n",
    "for i in range(N):\n",
    "    actual_label, predicted_label = predictImageLabel([], None)\n",
    "    best_move = findBestMove(predicted_label)\n",
    "\n",
    "    if((actual_label == 0 and best_move == 2) or (actual_label == 1 and best_move == 0) or (actual_label == 2 and best_move == 1)):\n",
    "        gain += 2\n",
    "    elif(actual_label != best_move): \n",
    "        gain -= 1\n",
    "    \n",
    "    gain_list.append(gain)\n",
    "\n",
    "    print(f\"Actual label: {actual_label}, Predicted label: {predicted_label}, Best move: {best_move}\")\n",
    "print(f\"Total gain: {gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(gain_list)), gain_list)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Total gain on each game')\n",
    "plt.title('Bar Plot of Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images outside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current class_path is dataset/test\\rock\n",
      "current class_path is dataset/test\\scissors\n",
      "current class_path is dataset/test\\paper\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 0, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Actual label: 1, Predicted label: 2, Best move: 1\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Actual label: 2, Predicted label: 2, Best move: 1\n",
      "Total gain: 1\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'dataset/test'\n",
    "images, labels = load_images(folder_path, (width,height))\n",
    "images, labels\n",
    "\n",
    "gain = 0\n",
    "gain_list = []\n",
    "n_components = 600\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image = cv2.resize(images[i], (20,30))\n",
    "    actual_label, predicted_label = predictImageLabel(image, labels[i])\n",
    "    best_move = findBestMove(predicted_label)\n",
    "\n",
    "    if((actual_label == 0 and best_move == 2) or (actual_label == 1 and best_move == 0) or (actual_label == 2 and best_move == 1)):\n",
    "        gain += 2\n",
    "    elif(actual_label != best_move): \n",
    "        gain -= 1\n",
    "    \n",
    "    gain_list.append(gain)\n",
    "\n",
    "    print(f\"Actual label: {actual_label}, Predicted label: {predicted_label}, Best move: {best_move}\")\n",
    "print(f\"Total gain: {gain}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
